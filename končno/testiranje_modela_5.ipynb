{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "#import dataset\n",
    "location = \"../Final_dataset/\"\n",
    "\n",
    "test = datasets.load_dataset(\"csv\", data_files=location + \"test_atomic.tsv\", delimiter=\"\\t\")\n",
    "test = test.rename_column(\"label\", \"labels\")\n",
    "test = test['train']\n",
    "\n",
    "#specil tokens\n",
    "PREMISE_SPECIAL = \"[PREMISE]\"\n",
    "HYPOTHESIS_SPECIAL = \"[HYPOTHESIS]\"\n",
    "\n",
    "PREMISE_ADDITIONAL_DESCRIPTION = \"[PREMISE_ADDITIONAL_DESCRIPTION]\"\n",
    "HYPOTHESIS_ADDITIONAL_DESCRIPTION = \"[HYPOTHESIS_ADDITIONAL_DESCRIPTION]\"\n",
    "\n",
    "id2label = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "#change the labels to integers\n",
    "test = test.map(lambda example: {\"labels\": label2id[example[\"labels\"]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#import finetuned SloBert model\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BatchEncoding\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "model_dir_normal = \"train/sloberta/sloberta-finetuned\"\n",
    "model_dir_atomic = \"train/sloberta/checkpoint-1500\"\n",
    "\n",
    "#gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir_normal, batched=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir_normal)\n",
    "model.to(device)\n",
    "\n",
    "tokenizer_atomic = AutoTokenizer.from_pretrained(model_dir_atomic, batched=True)\n",
    "model_atomic = AutoModelForSequenceClassification.from_pretrained(model_dir_atomic)\n",
    "model_atomic.to(device)\n",
    "\n",
    "#PREPROCESS FUNCTION\n",
    "def preprocess_function(examples):\n",
    "\t\"\"\" inputs = [f\"{PREMISE_SPECIAL} {prem} {HYPOTHESIS_SPECIAL} {hyp}\"\n",
    "\t\t\t\tfor prem, hyp in zip(examples[\"premise\"], examples[\"hypothesis\"])] \"\"\"\n",
    "\tinput = f\"{PREMISE_SPECIAL} {examples['premise']} {HYPOTHESIS_SPECIAL} {examples['hypothesis']}\"\n",
    "\tmodel_input = tokenizer(input, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "\t#labels in integers\n",
    "\t#model_inputs[\"labels\"] = tf.convert_to_tensor(examples[\"labels\"], dtype=tf.int64)\n",
    "\n",
    "\treturn model_input.to(device)\n",
    "\n",
    "def preprocess_function_atomic(example):\n",
    "\t\"\"\" inputs = [f\"{PREMISE_SPECIAL} {prem} {PREMISE_ADDITIONAL_DESCRIPTION} {prem_add_desc} {HYPOTHESIS_SPECIAL} {hyp} {HYPOTHESIS_ADDITIONAL_DESCRIPTION} {hyp_add_desc}\"\n",
    "\t\t\t\tfor prem, prem_add_desc, hyp, hyp_add_desc in zip(examples[\"premise\"], examples[\"premise_atomic\"], examples[\"hypothesis\"], examples[\"hypothesis_atomic\"])] \"\"\"\n",
    "\tinput = f\"{PREMISE_SPECIAL} {example['premise']} {PREMISE_ADDITIONAL_DESCRIPTION} {example['premise_atomic']} {HYPOTHESIS_SPECIAL} {example['hypothesis']} {HYPOTHESIS_ADDITIONAL_DESCRIPTION} {example['hypothesis_atomic']}\"\n",
    "\tmodel_input = tokenizer_atomic(input, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "\t#labels in integers\n",
    "\t#model_inputs[\"labels\"] = tf.convert_to_tensor(examples[\"labels\"], dtype=tf.int64)\n",
    "\n",
    "\treturn model_input.to(device)\n",
    "\n",
    "#preprocess the data\n",
    "\"\"\" test_prep = test.map(preprocess_function, batched=True)\n",
    "test_prep = test_prep.remove_columns(['premise', 'hypothesis', 'premise_atomic', 'hypothesis_atomic', 'labels'])\n",
    "\n",
    "test_prep_atomic = test.map(preprocess_function_atomic, batched=True)\n",
    "test_prep_atomic = test_prep_atomic.remove_columns(['premise', 'hypothesis', 'premise_atomic', 'hypothesis_atomic', 'labels']) \"\"\"\n",
    "test_prep = []\n",
    "test_prep_atomic = []\n",
    "for i in range(len(test)):\n",
    "\ttest_prep.append(preprocess_function(test[i]))\n",
    "\ttest_prep_atomic.append(preprocess_function_atomic(test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0/547\n",
      "Progress: 100/547\n",
      "Progress: 200/547\n",
      "Progress: 300/547\n",
      "Progress: 400/547\n",
      "Progress: 500/547\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model\n",
    "answers = []\n",
    "answers_atomic = []\n",
    "\n",
    "for i in range(len(test_prep)):\n",
    "\toutputs = model(**test_prep[i])\n",
    "\tpredictions = torch.argmax(outputs.logits, dim=1)\n",
    "\tanswers.append(predictions.tolist()[0])\n",
    "\n",
    "\toutputs_atomic = model_atomic(**test_prep_atomic[i])\n",
    "\tpredictions_atomic = torch.argmax(outputs_atomic.logits, dim=1)\n",
    "\tanswers_atomic.append(predictions_atomic.tolist()[0])\n",
    "\n",
    "\t#progress\n",
    "\tif i % 100 == 0:\n",
    "\t\tprint(f\"Progress: {i}/{len(test_prep)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 0, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 2, 0, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 0, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 0, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 0, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Accuracy: 0.3820840950639854\n",
      "Accuracy Atomic: 0.35283363802559414\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(predictions):\n",
    "\taccuracy = 0\n",
    "\tfor i in range(len(predictions)):\n",
    "\t\tif predictions[i] == test[i][\"labels\"]:\n",
    "\t\t\taccuracy += 1\n",
    "\treturn accuracy / len(predictions)\n",
    "\n",
    "\"\"\" print(answers)\n",
    "print(answers_atomic) \"\"\"\n",
    "accuracy = compute_accuracy(answers)\n",
    "accuracy_atomic = compute_accuracy(answers_atomic)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Accuracy Atomic: {accuracy_atomic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[0.0971, 0.0144, 0.0744]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1926,  0.2375, -0.5317]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "#atomic\n",
    "test = tokenizer_atomic(\"we'sopfsdffs 'čldskfčdlkdsfč'dfšosdfs podfsf\", return_tensors=\"pt\").to(device)\n",
    "test2 = tokenizer(\"qćććććsadsdasdeqweqwewqwe adflskjčs eqweqweeesfsdlkfčsdf sdšofksdfeee\", return_tensors=\"pt\").to(device)\n",
    "outputs = model_atomic(**test)\n",
    "outputs2 = model(**test2)\n",
    "print(outputs)\n",
    "print(outputs2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
