{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20/20 [00:00<00:00, 2237.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "#import dataset\n",
    "location = \"../končno/\"\n",
    "\n",
    "test = datasets.load_dataset(\"csv\", data_files=location + \"zadnje_testiranje.tsv\", delimiter=\"\\t\")\n",
    "test = test['train']\n",
    "\n",
    "#specil tokens\n",
    "PREMISE_SPECIAL = \"[PREMISE]\"\n",
    "HYPOTHESIS_SPECIAL = \"[HYPOTHESIS]\"\n",
    "\n",
    "PREMISE_ADDITIONAL_DESCRIPTION = \"[PREMISE_ADDITIONAL_DESCRIPTION]\"\n",
    "HYPOTHESIS_ADDITIONAL_DESCRIPTION = \"[HYPOTHESIS_ADDITIONAL_DESCRIPTION]\"\n",
    "\n",
    "id2label = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "#change the labels to integers\n",
    "test = test.map(lambda example: {\"labels\": label2id[example[\"labels\"]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#import finetuned SloBert model\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BatchEncoding\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "model_dir_normal = \"train/sloberta/sloberta-finetuned\"\n",
    "model_dir_atomic = \"train/sloberta/sloberta-atomic-finetuned\"\n",
    "model_dir_atomic = \"train/sloberta/sloberta-finetuned\"\n",
    "\n",
    "#gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir_normal, batched=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir_normal)\n",
    "#model.to(device)\n",
    "\n",
    "tokenizer_atomic = AutoTokenizer.from_pretrained(model_dir_atomic, batched=True)\n",
    "model_atomic = AutoModelForSequenceClassification.from_pretrained(model_dir_atomic)\n",
    "#model_atomic.to(device)\n",
    "\n",
    "#PREPROCESS FUNCTION\n",
    "def preprocess_function(examples):\n",
    "\t\"\"\" inputs = [f\"{PREMISE_SPECIAL} {prem} {HYPOTHESIS_SPECIAL} {hyp}\"\n",
    "\t\t\t\tfor prem, hyp in zip(examples[\"premise\"], examples[\"hypothesis\"])] \"\"\"\n",
    "\tinput = f\"{PREMISE_SPECIAL} {examples['premise']} {HYPOTHESIS_SPECIAL} {examples['hypothesis']}\"\n",
    "\tmodel_input = tokenizer(input, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "\t#labels in integers\n",
    "\t#model_inputs[\"labels\"] = tf.convert_to_tensor(examples[\"labels\"], dtype=tf.int64)\n",
    "\n",
    "\t#return model_input.to(device)\n",
    "\treturn model_input\n",
    "\n",
    "def preprocess_function_atomic(example):\n",
    "\t\"\"\" inputs = [f\"{PREMISE_SPECIAL} {prem} {PREMISE_ADDITIONAL_DESCRIPTION} {prem_add_desc} {HYPOTHESIS_SPECIAL} {hyp} {HYPOTHESIS_ADDITIONAL_DESCRIPTION} {hyp_add_desc}\"\n",
    "\t\t\t\tfor prem, prem_add_desc, hyp, hyp_add_desc in zip(examples[\"premise\"], examples[\"premise_atomic\"], examples[\"hypothesis\"], examples[\"hypothesis_atomic\"])] \"\"\"\n",
    "\tinput = f\"{PREMISE_SPECIAL} {example['premise']} {PREMISE_ADDITIONAL_DESCRIPTION} {example['premise_atomic']} {HYPOTHESIS_SPECIAL} {example['hypothesis']} {HYPOTHESIS_ADDITIONAL_DESCRIPTION} {example['hypothesis_atomic']}\"\n",
    "\tmodel_input = tokenizer_atomic(input, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "\t#labels in integers\n",
    "\t#model_inputs[\"labels\"] = tf.convert_to_tensor(examples[\"labels\"], dtype=tf.int64)\n",
    "\n",
    "\t#return model_input.to(device)\n",
    "\treturn model_input\n",
    "\n",
    "#preprocess the data\n",
    "\"\"\" test_prep = test.map(preprocess_function, batched=True)\n",
    "test_prep = test_prep.remove_columns(['premise', 'hypothesis', 'premise_atomic', 'hypothesis_atomic', 'labels'])\n",
    "\n",
    "test_prep_atomic = test.map(preprocess_function_atomic, batched=True)\n",
    "test_prep_atomic = test_prep_atomic.remove_columns(['premise', 'hypothesis', 'premise_atomic', 'hypothesis_atomic', 'labels']) \"\"\"\n",
    "test_prep = []\n",
    "test_prep_atomic = []\n",
    "for i in range(len(test)):\n",
    "\ttest_prep.append(preprocess_function(test[i]))\n",
    "\ttest_prep_atomic.append(preprocess_function_atomic(test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0/20\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model\n",
    "answers = []\n",
    "answers_atomic = []\n",
    "\n",
    "for i in range(len(test_prep)):\n",
    "\toutputs = model(**test_prep[i])\n",
    "\tpredictions = torch.argmax(outputs.logits, dim=1)\n",
    "\tanswers.append(predictions.tolist()[0])\n",
    "\n",
    "\toutputs_atomic = model_atomic(**test_prep_atomic[i])\n",
    "\tpredictions_atomic = torch.argmax(outputs_atomic.logits, dim=1)\n",
    "\tanswers_atomic.append(predictions_atomic.tolist()[0])\n",
    "\n",
    "\t#progress\n",
    "\tif i % 100 == 0:\n",
    "\t\tprint(f\"Progress: {i}/{len(test_prep)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n",
      "[1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1]\n",
      "Accuracy: 0.4\n",
      "Accuracy Atomic: 0.45\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(predictions):\n",
    "\taccuracy = 0\n",
    "\tfor i in range(len(predictions)):\n",
    "\t\tif predictions[i] == test[i][\"labels\"]:\n",
    "\t\t\taccuracy += 1\n",
    "\treturn accuracy / len(predictions)\n",
    "\n",
    "print(answers)\n",
    "print(answers_atomic)\n",
    "accuracy = compute_accuracy(answers)\n",
    "accuracy_atomic = compute_accuracy(answers_atomic)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Accuracy Atomic: {accuracy_atomic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#atomic\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer_atomic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msopfsdffs \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mčldskfčdlkdsfč\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdfšosdfs podfsf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m test2 \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqćććććsadsdasdeqweqwewqwe adflskjčs eqweqweeesfsdlkfčsdf sdšofksdfeee\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model_atomic(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtest)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:789\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m--> 789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    791\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:789\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m--> 789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    791\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "#atomic\n",
    "test = tokenizer_atomic(\"we'sopfsdffs 'čldskfčdlkdsfč'dfšosdfs podfsf\", return_tensors=\"pt\").to(device)\n",
    "test2 = tokenizer(\"qćććććsadsdasdeqweqwewqwe adflskjčs eqweqweeesfsdlkfčsdf sdšofksdfeee\", return_tensors=\"pt\").to(device)\n",
    "outputs = model_atomic(**test)\n",
    "outputs2 = model(**test2)\n",
    "print(outputs)\n",
    "print(outputs2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
